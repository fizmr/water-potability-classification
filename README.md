# Water Potability Classification Project ğŸ’§

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Library](https://img.shields.io/badge/Library-Scikit_Learn%20%7C%20XGBoost%20%7C%20CatBoost-orange)
![Status](https://img.shields.io/badge/Status-Completed-success)

[ğŸ‡¬ğŸ‡§ English](#english) | [ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e](#tÃ¼rkÃ§e)

---

<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English

### 1. Project Overview
This project aims to predict the potability of water based on various water quality metrics (pH, Hardness, Solids, Chloramines, etc.). The goal is to classify water samples as **Potable (1)** or **Not Potable (0)** using machine learning techniques.

This repository consolidates two different analytical approaches to tackle the classification problem on a challenging dataset.

### 2. Dataset & Literature Review
**Dataset Context:** The dataset consists of 3276 water samples.
**Challenge:** This dataset is known in the machine learning community as "noisy" and "hard to classify" due to the low correlation between features and the target variable.
**Literature Alignment:** Similar studies in the literature typically achieve accuracy scores between **60% and 68%** without data leakage. The results obtained in this project (%62-%64) are consistent with real-world scenarios and literature benchmarks.

### 3. Key Preprocessing Techniques
Since the raw data was imbalanced and contained missing values, the following techniques were applied:

* **KNN Imputer:** Instead of filling missing values with the mean/median, the **K-Nearest Neighbors** algorithm was used to estimate values based on sample similarity, preserving data distribution.
* **SMOTE (Synthetic Minority Over-sampling Technique):** The dataset was imbalanced (fewer "Potable" samples). SMOTE was used to generate synthetic samples for the minority class to prevent model bias.
* **Scaling:** Features were normalized using StandardScaler to ensure distance-based algorithms work correctly.

---

### ğŸ“Š [Part 1: XGBoost & Random Forest Approach](xgb-rf/waterPotabilityClassification_xgb-rf.ipynb)
*(Click the title above to view the code)*

In this section, tree-based ensemble methods were utilized.

#### A. Exploratory Data Analysis (EDA)
Before modeling, the data was visualized to understand relationships.

1.  **Correlation Heatmap:**
    This heatmap shows the relationship between features. The low correlation values indicate that no single feature is enough to determine potability, necessitating complex non-linear models.
    > ![Correlation Heatmap](xgb-rf/images/kolerasyon.png)

2.  **Class Distribution:**
    Visualizing the target variable balance before and after SMOTE.
    > ![Class Distribution](xgb-rf/images/iÃ§ilebilirlik.png)

3.  **Feature Distribution:**
    Boxplots and histograms showing how features like pH and Hardness vary between Potable and Not Potable classes.
    > ![Feature Distribution](xgb-rf/images/daÄŸÄ±lÄ±m.png)

#### B. Model Performance (Part 1)

**1. Random Forest Classifier**
Random Forest provided the most stable results among tree-based models.

* **Accuracy:** **63.72%**
* **Confusion Matrix:**
    > ![RF Confusion Matrix](xgb-rf/images/rf_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.70      0.72      0.71       400
           1       0.54      0.51      0.52       256

    accuracy                           0.64       656
   macro avg       0.62      0.61      0.61       656
weighted avg       0.63      0.64      0.64       656
```

**2. XGBoost Classifier**
XGBoost was tuned for gradient boosting performance.

* **Accuracy:** **62.50%**
* **Confusion Matrix:**
    > ![RF Confusion Matrix](xgb-rf/images/xgb_confmat.png)

```text
               precision    recall  f1-score   support

           0       0.70      0.68      0.69       400
           1       0.52      0.55      0.53       256

    accuracy                           0.62       656
   macro avg       0.61      0.61      0.61       656
weighted avg       0.63      0.62      0.63       656
```
#### C. Evaluation (Part 1)

* **ROC-AUC Curve:** Comparison of the models' ability to distinguish classes.
  > ![ROC-AUC CURVE](xgb-rf/images/roc-pred.png)
---

### ğŸš€ [Part 2: CatBoost, SVM & MLP Approach](catb-svm-mlp/waterPotabilityClassification_catb-svm-mlp.ipynb)
*(Click the title above to view the code)*

In this second part of the project, we explored different algorithms (**CatBoost, SVM, and MLP**) and applied a different preprocessing strategy compared to Part 1 to observe the impact on model performance.

#### A. Preprocessing & Feature Engineering (Specific to Part 2)
Unlike Part 1, the following techniques were applied here:
* **Mean Imputation:** Missing values were filled using the mean strategy instead of KNN.
* **Feature Engineering:** New synthetic features were created to capture hidden relationships between water quality metrics.
* **Scaling:** Data was scaled (StandardScaler/MinMaxScaler) which is crucial for distance-based models like SVM and Neural Networks (MLP).

#### B. Exploratory Data Analysis (Part 2)
Since the feature set changed due to engineering and imputation methods, the correlations were re-evaluated.

1.  **Correlation Heatmap (Post-Engineering):**
    Visualizing the relationships after adding new features and mean imputation.
    > ![Part 2 Heatmap](catb-svm-mlp/images/kolerasyon.png)

2.  **Feature Distribution:**
    > ![Part 2 Feature Distribution](catb-svm-mlp/images/daÄŸÄ±lÄ±m.png)

#### C. Model Performance (Part 2)

**1. MLP Classifier (Multi-Layer Perceptron)**
The Artificial Neural Network (MLP) achieved the **highest accuracy** across all models in this project.

* **Accuracy:** **66.01%**
* **Confusion Matrix:**
    > ![MLP Confusion Matrix](catb-svm-mlp/images/mlp_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.71      0.76      0.73       400
           1       0.57      0.50      0.54       256

    accuracy                           0.66       656
   macro avg       0.64      0.63      0.63       656
weighted avg       0.65      0.66      0.66       656
```
**2. CatBoost Classifier**
A robust gradient boosting algorithm that handles categorical data well.
* **Accuracy:** **62.80%**
* **Confusion Matrix:**
  > ![CatBoost Confusion Matrix](catb-svm-mlp/images/catb_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.68      0.72      0.70       400
           1       0.53      0.48      0.50       256

    accuracy                           0.63       656
   macro avg       0.60      0.60      0.60       656
weighted avg       0.62      0.63      0.62       656
```
**3. SVM (Support Vector Machine)**
SVM struggled slightly compared to ensemble and neural network methods on this specific dataset.
* **Accuracy:** **61.12%**
* **Confusion Matrix:**
  > ![SVM Confusion Matrix](catb-svm-mlp/images/svm_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.67      0.70      0.69       400
           1       0.50      0.47      0.49       256

    accuracy                           0.61       656
   macro avg       0.59      0.59      0.59       656
weighted avg       0.61      0.61      0.61       656
```

#### D. Final Evaluation & Visuals (Part 2)
* **ROC-AUC Performance:** Evaluating the True Positive vs False Positive rates for the new models.
  > ![ROC-AUC CURVE](catb-svm-mlp/images/roc-pred.png)

### ğŸ† Model Performances & Results

Comparing all 5 models developed across both notebooks, the MLP (Neural Network) provided the best ability to classify water potability, followed closely by Random Forest.

| Rank | Model | Accuracy | Approach |
| :--- | :---: | :---: | :---: |
| ğŸ¥‡ | **MLP (Neural Network)** | 66.01% | Part 2 |
| ğŸ¥ˆ | **Random Forest** | 63.72% | Part 1 | 
| ğŸ¥‰ | **CatBoost** | 62.80% | Part 2 |
| 4 | **XGBoost** | 62.50% | Part 1 |
| 5 | **SVM** | 61.13% | Part 2 |

**Conclusion:**
While the dataset is challenging with low feature correlation, the MLP model with Feature Engineering and Mean Imputation yielded the most promising results for this classification task. 

---

<a name="tÃ¼rkÃ§e"></a>
## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e

### 1. Proje Genel BakÄ±ÅŸÄ±
Bu proje, Ã§eÅŸitli su kalitesi metriklerine (pH, Sertlik, KatÄ± Maddeler, Kloraminler vb.) dayanarak suyun iÃ§ilebilirliÄŸini tahmin etmeyi amaÃ§lamaktadÄ±r. Hedef, makine Ã¶ÄŸrenimi tekniklerini kullanarak su Ã¶rneklerini **Ä°Ã§ilebilir (1)** veya **Ä°Ã§ilemez (0)** olarak sÄ±nÄ±flandÄ±rmaktÄ±r.

Bu depo, zorlu bir veri setindeki sÄ±nÄ±flandÄ±rma problemini ele almak iÃ§in iki farklÄ± analitik yaklaÅŸÄ±mÄ± birleÅŸtirmektedir.

### 2. Veri Seti ve LiteratÃ¼r TaramasÄ±
**Veri Seti BaÄŸlamÄ±:** Veri seti 3276 su Ã¶rneÄŸinden oluÅŸmaktadÄ±r.
**Zorluk:** Bu veri seti, Ã¶zellikler ve hedef deÄŸiÅŸken arasÄ±ndaki dÃ¼ÅŸÃ¼k korelasyon nedeniyle makine Ã¶ÄŸrenimi topluluÄŸunda "gÃ¼rÃ¼ltÃ¼lÃ¼" ve "sÄ±nÄ±flandÄ±rÄ±lmasÄ± zor" olarak bilinir.
**LiteratÃ¼r Uyumu:** LiteratÃ¼rdeki benzer Ã§alÄ±ÅŸmalar, veri sÄ±zÄ±ntÄ±sÄ± olmadan genellikle **%60 ile %68** arasÄ±nda doÄŸruluk skorlarÄ± elde etmektedir. Bu projede elde edilen sonuÃ§lar (%62-%64), gerÃ§ek dÃ¼nya senaryolarÄ± ve literatÃ¼r kÄ±yaslamalarÄ±yla tutarlÄ±dÄ±r.

### 3. Temel Ã–n Ä°ÅŸleme Teknikleri
Ham veri dengesiz olduÄŸundan ve eksik deÄŸerler iÃ§erdiÄŸinden, aÅŸaÄŸÄ±daki teknikler uygulanmÄ±ÅŸtÄ±r:

* **KNN Imputer:** Eksik deÄŸerleri ortalama/medyan ile doldurmak yerine, veri daÄŸÄ±lÄ±mÄ±nÄ± koruyarak Ã¶rnek benzerliÄŸine dayalÄ± deÄŸerleri tahmin etmek iÃ§in **K-En YakÄ±n KomÅŸu (K-Nearest Neighbors)** algoritmasÄ± kullanÄ±ldÄ±.
* **SMOTE (Sentetik AzÄ±nlÄ±k AÅŸÄ±rÄ± Ã–rnekleme TekniÄŸi):** Veri seti dengesizdi (daha az "Ä°Ã§ilebilir" Ã¶rnek mevcuttu). Model yanlÄ±lÄ±ÄŸÄ±nÄ± Ã¶nlemek amacÄ±yla azÄ±nlÄ±k sÄ±nÄ±fÄ± iÃ§in sentetik Ã¶rnekler oluÅŸturmak Ã¼zere SMOTE kullanÄ±ldÄ±.
* **Ã–lÃ§eklendirme (Scaling):** Mesafe tabanlÄ± algoritmalarÄ±n doÄŸru Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlamak iÃ§in Ã¶zellikler StandardScaler kullanÄ±larak normalleÅŸtirildi.

---

### ğŸ“Š [BÃ¶lÃ¼m 1: XGBoost & Random Forest YaklaÅŸÄ±mÄ±](xgb-rf/waterPotabilityClassification_xgb-rf.ipynb)
*(Koda gitmek iÃ§in yukarÄ±daki baÅŸlÄ±ÄŸa tÄ±klayÄ±n)*

Bu bÃ¶lÃ¼mde, aÄŸaÃ§ tabanlÄ± topluluk (ensemble) yÃ¶ntemleri kullanÄ±lmÄ±ÅŸtÄ±r.

#### A. KeÅŸifsel Veri Analizi (EDA)
Modelleme Ã¶ncesinde, iliÅŸkileri anlamak iÃ§in veri gÃ¶rselleÅŸtirildi.

1.  **Korelasyon IsÄ± HaritasÄ±:**
    Bu Ä±sÄ± haritasÄ± Ã¶zellikler arasÄ±ndaki iliÅŸkiyi gÃ¶sterir. DÃ¼ÅŸÃ¼k korelasyon deÄŸerleri, iÃ§ilebilirliÄŸi belirlemek iÃ§in tek bir Ã¶zelliÄŸin yeterli olmadÄ±ÄŸÄ±nÄ±, bu nedenle karmaÅŸÄ±k doÄŸrusal olmayan modellere ihtiyaÃ§ duyulduÄŸunu gÃ¶sterir.
    > ![Korelasyon IsÄ± HaritasÄ±](xgb-rf/images/kolerasyon.png)

2.  **SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:**
    SMOTE Ã¶ncesi ve sonrasÄ± hedef deÄŸiÅŸken dengesinin gÃ¶rselleÅŸtirilmesi.
    > ![SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±](xgb-rf/images/iÃ§ilebilirlik.png)

3.  **Ã–zellik DaÄŸÄ±lÄ±mÄ±:**
    pH ve Sertlik gibi Ã¶zelliklerin Ä°Ã§ilebilir ve Ä°Ã§ilemez sÄ±nÄ±flarÄ± arasÄ±nda nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶steren kutu grafikleri (boxplots) ve histogramlar.
    > ![Ã–zellik DaÄŸÄ±lÄ±mÄ±](xgb-rf/images/daÄŸÄ±lÄ±m.png)

#### B. Model PerformansÄ± (BÃ¶lÃ¼m 1)

**1. Random Forest SÄ±nÄ±flandÄ±rÄ±cÄ±**
Random Forest, aÄŸaÃ§ tabanlÄ± modeller arasÄ±nda en kararlÄ± sonuÃ§larÄ± saÄŸladÄ±.

* **DoÄŸruluk (Accuracy):** **%63.72**
* **KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix):**
    > ![RF Confusion Matrix](xgb-rf/images/rf_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.70      0.72      0.71       400
           1       0.54      0.51      0.52       256

    accuracy                           0.64       656
   macro avg       0.62      0.61      0.61       656
weighted avg       0.63      0.64      0.64       656
```

**2. XGBoost SÄ±nÄ±flandÄ±rÄ±cÄ±**
XGBoost, gradient boosting performansÄ± iÃ§in ayarlandÄ±.

* **DoÄŸruluk (Accuracy):** **%62.50**
* **KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix):**
    > ![RF Confusion Matrix](xgb-rf/images/xgb_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.70      0.68      0.69       400
           1       0.52      0.55      0.53       256

    accuracy                           0.62       656
   macro avg       0.61      0.61      0.61       656
weighted avg       0.63      0.62      0.63       656
```
#### C. DeÄŸerlendirme (Part 1)

* **ROC-AUC EÄŸrisi:** Modellerin sÄ±nÄ±flarÄ± ayÄ±rt etme yeteneÄŸinin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±.
  > ![ROC-AUC CURVE](xgb-rf/images/roc-pred.png)
---

### ğŸš€ [BÃ¶lÃ¼m 2: CatBoost, SVM & MLP YaklaÅŸÄ±mÄ±](catb-svm-mlp/waterPotabilityClassification_catb-svm-mlp.ipynb)
*(Koda gitmek iÃ§in yukarÄ±daki baÅŸlÄ±ÄŸa tÄ±klayÄ±n)*

Projenin bu ikinci bÃ¶lÃ¼mÃ¼nde, farklÄ± algoritmalar (**CatBoost, SVM ve MLP**) denenmiÅŸ ve BÃ¶lÃ¼m 1'den farklÄ± bir Ã¶n iÅŸleme stratejisi uygulanarak model performansÄ±na etkisi gÃ¶zlemlenmiÅŸtir.

#### A. Ã–n Ä°ÅŸleme ve Ã–zellik MÃ¼hendisliÄŸi (BÃ¶lÃ¼m 2'ye Ã–zel)
BÃ¶lÃ¼m 1'in aksine, bu Ã§alÄ±ÅŸmada ÅŸu teknikler uygulanmÄ±ÅŸtÄ±r:
* **Ortalama ile Doldurma (Mean Imputation):** Eksik veriler KNN yerine ortalama deÄŸerler kullanÄ±larak doldurulmuÅŸtur.
* **Ã–zellik MÃ¼hendisliÄŸi (Feature Engineering):** Su kalite metrikleri arasÄ±ndaki gizli iliÅŸkileri yakalamak iÃ§in yeni (sentetik) sÃ¼tunlar oluÅŸturulmuÅŸtur.
* **Ã–lÃ§eklendirme:** SVM ve Sinir AÄŸlarÄ± (MLP) gibi uzaklÄ±k tabanlÄ± modellerin baÅŸarÄ±sÄ± iÃ§in veriler Ã¶lÃ§eklendirilmiÅŸtir (Scaling).

#### B. KeÅŸifsel Veri Analizi (BÃ¶lÃ¼m 2)
Ã–zellik mÃ¼hendisliÄŸi ve farklÄ± doldurma yÃ¶ntemleri nedeniyle veri seti deÄŸiÅŸtiÄŸi iÃ§in korelasyonlar yeniden deÄŸerlendirilmiÅŸtir.

1.  **Korelasyon IsÄ± HaritasÄ± (Heatmap):**
    Yeni Ã¶zelliklerin eklenmesi ve ortalama ile doldurma iÅŸlemi sonrasÄ± iliÅŸkilerin gÃ¶rselleÅŸtirilmesi.
    > ![Part 2 Heatmap](catb-svm-mlp/images/kolerasyon.png)

2.  **Ã–zellik DaÄŸÄ±lÄ±m GrafiÄŸi:**
    > ![Part 2 Ã–zellik DaÄŸÄ±lÄ±mÄ±](catb-svm-mlp/images/daÄŸÄ±lÄ±m.png)
    
#### C. Model PerformansÄ± (BÃ¶lÃ¼m 2)

**1. MLP Classifier (Yapay Sinir AÄŸÄ±)**
MLP, tÃ¼m proje genelinde denenen modeller arasÄ±nda **en yÃ¼ksek doÄŸruluÄŸu** saÄŸlayan model olmuÅŸtur.

* **DoÄŸruluk (Accuracy):** **%66.01**
* **KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix):**
    > ![MLP Confusion Matrix](catb-svm-mlp/images/mlp_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.71      0.76      0.73       400
           1       0.57      0.50      0.54       256

    accuracy                           0.66       656
   macro avg       0.64      0.63      0.63       656
weighted avg       0.65      0.66      0.66       656
```
**2. CatBoost Classifier**
Kategorik verilerle gÃ¼Ã§lÃ¼ Ã§alÄ±ÅŸan popÃ¼ler bir gradient boosting algoritmasÄ±dÄ±r.
* **DoÄŸruluk:** **62.80%**
* **Confusion Matrix:**
  > ![CatBoost Confusion Matrix](catb-svm-mlp/images/catb_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.68      0.72      0.70       400
           1       0.53      0.48      0.50       256

    accuracy                           0.63       656
   macro avg       0.60      0.60      0.60       656
weighted avg       0.62      0.63      0.62       656
```
**3. SVM (Support Vector Machine)**
SVM, bu veri setinde topluluk (ensemble) ve sinir aÄŸÄ± yÃ¶ntemlerine kÄ±yasla daha dÃ¼ÅŸÃ¼k bir performans gÃ¶stermiÅŸtir.
* **DoÄŸruluk:** **61.12%**
* **Confusion Matrix:**
  > ![SVM Confusion Matrix](catb-svm-mlp/images/svm_confmat.png)

```text
              precision    recall  f1-score   support

           0       0.67      0.70      0.69       400
           1       0.50      0.47      0.49       256

    accuracy                           0.61       656
   macro avg       0.59      0.59      0.59       656
weighted avg       0.61      0.61      0.61       656
```
#### D. Final Evaluation & Visuals (Part 2)
* **ROC-AUC Performance:** Modellerin doÄŸru pozitif ve yanlÄ±ÅŸ pozitif oranlarÄ±nÄ±n karÅŸÄ±laÅŸtÄ±rmalÄ± eÄŸrisi.
  > ![ROC-AUC EÄŸrisi](catb-svm-mlp/images/roc-pred.png)

### ğŸ† Proje Sonucu: TÃ¼m Modellerin KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±

Ä°ki farklÄ± dosyada geliÅŸtirilen 5 model karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda, MLP (Yapay Sinir AÄŸÄ±) suyun iÃ§ilebilirliÄŸini tahmin etmede en baÅŸarÄ±lÄ± model olmuÅŸtur.

| SÄ±ralama | Model | DoÄŸruluk | KÄ±sÄ±m |
| :--- | :---: | :---: | :---: |
| ğŸ¥‡ | **MLP (Neural Network)** | 66.01% | BÃ¶lÃ¼m 2 |
| ğŸ¥ˆ | **Random Forest** | 63.72% | BÃ¶lÃ¼m 1 | 
| ğŸ¥‰ | **CatBoost** | 62.80% | BÃ¶lÃ¼m 2 |
| 4 | **XGBoost** | 62.50% | BÃ¶lÃ¼m 1 |
| 5 | **SVM** | 61.13% | BÃ¶lÃ¼m 2 |

**SonuÃ§:**
Veri seti dÃ¼ÅŸÃ¼k korelasyonlu zor bir yapÄ±ya sahip olsa da, Ã–zellik MÃ¼hendisliÄŸi ve Ortalama ile Doldurma tekniklerini kullanan MLP modeli, bu sÄ±nÄ±flandÄ±rma problemi iÃ§in en umut verici sonuÃ§larÄ± Ã¼retmiÅŸtir.
